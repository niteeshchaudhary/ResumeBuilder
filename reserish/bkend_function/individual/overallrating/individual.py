import PyPDF2
import json
import re
from groq import Groq
import os
from dotenv import load_dotenv
from pathlib import Path
from youtubesearchpython import VideosSearch
import docx2txt
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from .groq_api_manager import groq_manager

sample_resume_json = """{
    name: "",
    phonenumber: "",
    email: "",
    linkedin: "",
    github: "",
    portfolio: "",
    emailthumbnail: "",
    linkedinthumbnail: "",
    githubthumbnail: "",
    portfoliothumbnail: "",
    summary: "",
    skills: {
      "Programming Languages": [],
      "Tools & Frameworks": [],
      "Soft Skills": []
    },
    experiences: [
      {
        company: "",
        role: "",
        from_date: "",
        to_date: "",
        location: "",
        currentlyWorking: false,
        details: ["", ""]
      }
    ],
    education: [
      {
        institution: "",
        from_date: "",
        to_date: "",
        degree: "",
        fieldOfStudy: "",
        scoreType:"",
        currentlyStudying: false,
        score: "",
        coursework: ["", ""]
      }
    ],
    projects: [
      {
        title: "",
        technologies: "",
        from_date: new Date(),
        to_date: new Date(),
        details: ["", ""],
        projectLink: ""
      }
    ],
    certifications: [
      { name: "", link: "" }
    ],
    publications: [
      { title: "", link: "" }
    ],
    achievements: [
      { title: "", link: "" }
    ]
}"""

# Define function to extract text from PDF
def extract_text_from_pdf(file_path):
    try:
        with open(file_path, "rb") as file:
            reader = PyPDF2.PdfReader(file)
            text = "".join([page.extract_text() for page in reader.pages])
        return text
    except Exception:
        return ""

# Function to clean text by removing empty lines
def clean_text_docx(text):
    cleaned_lines = [line.strip() for line in text.splitlines() if line.strip()]
    return "\n".join(cleaned_lines)


# Function to extract and clean text from a DOCX file
def extract_text_from_docx2txt(docx_path):
    try:
        text = docx2txt.process(docx_path)
    except:
        text = ""
    return clean_text_docx(text)


# Define function to clean up text generated by LLM
def clean_text(text):
    text = re.sub(r'[^\x00-\x7F]+', '', text) 
    text = re.sub(r'\s+', ' ', text) 
    return text.strip()

# Define function to interact with LLama3 API
def query_llama3(prompt, token_size=100):
    try:
        response = groq_manager.make_request(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=token_size,
        )
        return clean_text(response.choices[0].message.content.strip())
    except Exception as e:
        logging.error(f"LLM query failed: {e}")
        return None
    

# Define functiontointeract with LLama3 API
# def query_llama3(prompt, token_size=100):
#     try:
#         response = client.chat.completions.create(
#             model="llama-3.3-70b-versatile",
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.5,
#             max_tokens=token_size,
#         )
#         return clean_text(response.choices[0].message.content.strip())
#     except Exception as e:
#         logging.error(f"LLM query failed: {e}")
#         return None


# Define functiontogenerate prompts for LLama3
def generate_prompts(resume_text):
    # and provide feedback:
    return {
        "is_resume_valid": f"do you recognise it a valid resume info? If not, just output 'NO' else output 'YES': \n{resume_text}",
        "get_resume_score": f"Rate this resume on a scale of 1 to 10 \n{resume_text}",
        "job_role_and_skills": f"Predict the most suitable job role based on this resume:\n{resume_text}",
        "suggest_courses_and_certificates": f"Suggest relevant courses and certificates based on this resume:\n{resume_text}",
        "get_resume_tips": f"Provide tips for improving the resume:\n{resume_text}",
    }

# Search for videos using keywords
def ytvideos(search_key):
    print(search_key)
    results={}
    try:
        videos_search = VideosSearch(f'{search_key} Interview Preparation Tips', limit=3)
        results = videos_search.result()
    except Exception as ex:
        print(ex)
    print(results)
    return [video['link'] for video in results.get('result',[])]

# Verify if a role is valid using LLM
def verify_job_role_with_llm(role, token_size):
    prompt = f"Is '{role}' a valid job role?"
    response = query_llama3(prompt, token_size)
    # print(response)
    if not response:
        print(prompt,"got false response")
        return False
    print(f"LLM response for role '{role}': {response}")
    return "yes" in response.lower() or "true" in response.lower()

# Extract job roles enclosed in double asterisks
def extract_job_roles(job_role_and_skills):
    roles = re.findall(r'\*\*(.*?)\*\*', job_role_and_skills)
    split_roles = []
    for role in roles:
        split_roles.extend([r.strip() for r in role.split('/') if r.strip()])
    return split_roles

def remove_incomplete_lines(text):
    text.replace("\n", "")  # Replace newlines with spaces
    # Remove incomplete lines from the text
    # split text on (., !, ?)
    lines= re.split(r'(?<=[.!?])\s+', text)
    return "\n".join([line for line in lines[:-1]])


    # lines = text.split('\n')  # Split text into lines
    # complete_lines = []

    # for line in lines:
    #     # Check if a line is complete by looking for a valid end punctuation
    #     if line.strip() and line[-1] in '.!?':  
    #         complete_lines.append(line)
    #     else:
    #         # If a line is incomplete and doesn't end with punctuation, ignore it
    #         break

    # # Join the valid lines back into a single text
    # return '\n'.join(complete_lines)


def rate_resume(job_role,resume_text):
    # if file_path.endswith('.pdf'):
    #     resume_text = extract_text_from_pdf(file_path)
    # elif file_path.endswith('.docx'):
    #     resume_text = extract_text_from_docx2txt(file_path)
    result=query_llama3("Rate this resume (only if its resume else give error message) on a scale of 1 to 10 on the basis of Weighted score based on: Job Role Match: 30%,Skills Match: 30%,Structure & Formatting: 20%,Grammar & Language: 10%,Overall Impression: 10% (output in json form {\"score\": Outputscore,\"tips\": Output tips to improve} and no extra text, make sure output is generated complete keep it short)" +f"on the basis of resume extracted from file, job role:  {job_role} (take given experience also in consideration for role designation),given resume data: {resume_text}",500)
    print(result)
    return result
    

# Process resume and get recommendations from LLama3
def process_resume(file_path, token_size):
    if file_path.endswith('.pdf'):
        resume_text = extract_text_from_pdf(file_path)
    elif file_path.endswith('.docx'):
        resume_text = extract_text_from_docx2txt(file_path)
    
    prompts = generate_prompts(resume_text)
    
    with ThreadPoolExecutor() as executor:
        llama_futures = {
            "is_resume_valid": executor.submit(query_llama3, prompts["is_resume_valid"], token_size),
            "job_role_and_skills": executor.submit(query_llama3, prompts["job_role_and_skills"], token_size),
            "courses_and_certificates": executor.submit(query_llama3, prompts["suggest_courses_and_certificates"], token_size),
            "resume_tips": executor.submit(query_llama3, prompts["get_resume_tips"], token_size),
            "resume_score": executor.submit(query_llama3, prompts["get_resume_score"], token_size),
        }


        results = {key: future.result() for key, future in llama_futures.items()}
        print(results)
        if not results["is_resume_valid"] or "no" in results["is_resume_valid"].lower():
            return {
                "Overall Resume Score (out of 10)": "Invalid Resume",
                "Job Role and Skills": "Invalid Resume",
                "Courses and Certificates": "Invalid Resume",
                "Resume Tips and Ideas": "Invalid Resume",
                "Interview Tips Video": "Invalid Resume",
            }
        
        job_roles = extract_job_roles(results["job_role_and_skills"])
        interview_videos = {}

        if job_roles:
            video_futures = {role: executor.submit(ytvideos, role) for role in job_roles[:2] if verify_job_role_with_llm(role, token_size)}
            for role, future in video_futures.items():
                print(f"Fetching videos for role: {role}")
                interview_videos[role] = future.result()
                print(f"Videos for {role}: {interview_videos[role]}")
        else:
            interview_videos = 'No relevant videos found'


    return {
        "Overall Resume Score (out of 10)":  results["resume_score"],
        "Job Role and Skills":  remove_incomplete_lines(results["job_role_and_skills"]),
        "Courses and Certificates":  remove_incomplete_lines(results["courses_and_certificates"]),
        "Resume Tips and Ideas": remove_incomplete_lines(results["resume_tips"]),
        "Interview Tips Video": interview_videos,
    }

# Save results to JSON
def save_results_to_json(results, output_file):
    with open(output_file, "w") as json_file:
        json.dump(results, json_file, indent=4)
    with open(output_file, "r") as json_file:
        print(json.dumps(json.load(json_file), indent=4))

def extract_first_point(text):
    match = re.search(r"1\.\s*(.*)", text)
    if match:
        return match.group(1)
    return None

def get_reprased_text(client_prompt,text,pastText,token_size=20):
    prompt=""
    if text=="":
        print(pastText)
        prompt = f"{client_prompt}.Only give thesentenceas output no extra text"
        try:
            response = groq_manager.make_request(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=token_size,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"LLM query failed: {e}")
            return "Error...."
    else:
        prompt = f"Rephrase the sentences for resume in short (1 line).Only give the sentenceas output no extra text: {text}"

    try:
        response = groq_manager.make_request(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=token_size,
        )
        print(response.choices[0].message.content.strip())
        return extract_first_point(response.choices[0].message.content.strip()) or response.choices[0].message.content.strip()

        # if json_match:
        #     # Extract the JSON string
        #     json_str = json_match.group(1)
        #     data = json.loads(json_str)
        #     print(json.dumps(data, indent=4))
        #     return data
        # return {}
        # return clean_text(response.choices[0].message.content.strip())
    except Exception as e:
        logging.error(f"LLM query failed: {e}")
        return "Error...."

def get_resume_json(file_path, token_size):
    results = extract_text_from_pdf(file_path).lower()
    # print(results.replace("currently working", "present"),"_*_*_*")
    # results.replace("currently working", "present")
    prompt = f"Get the given data convetered to json of type {sample_resume_json} :\n{results}"
    try:
        response = groq_manager.make_request(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=token_size,
        )
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response.choices[0].message.content.strip(), re.DOTALL)
        if json_match:
            # Extract the JSON string
            json_str = json_match.group(1)
            data = json.loads(json_str)
            
            data["emailthumbnail"] = data["email"]

            if not data["linkedin"].startswith("http"):
                data["linkedinthumbnail"] = data["linkedin"]
                data["linkedin"] = ""
            else:
                data["linkedinthumbnail"]  = data["linkedin"].split("/")[-1]

            if not data["github"].startswith("http"):
                data["githubthumbnail"] = data["github"]
                data["github"] = ""
            else:
                data["githubthumbnail"] = data["github"].split("/")[-1]

            if not data["portfolio"].startswith("http"):
                data["portfoliothumbnail"] = data["portfolio"]
                data["portfolio"] = ""
            else:
                data["portfoliothumbnail"] = data["portfolio"].split("/")[-1]

            print(json.dumps(data, indent=4))
            return data
        return {}
        # return clean_text(response.choices[0].message.content.strip())
    except Exception as e:
        logging.error(f"LLM query failed: {e}")
        return {}

if __name__ == "__main__":

    file_path = "resume_src/Niteesh_Resume.pdf"
    analysis = input(f'Choose any one either "Brief" or "Detail" Analysis: ')

    token_size = 256 if analysis == 'Brief' else 512
    results = process_resume(file_path, token_size)

    output_file = "resume_results.json"
    save_results_to_json(results, output_file)





